"""
Script de d√©monstration pour tester le mod√®le existant.
Charge le mod√®le d√©j√† entra√Æn√© et fait des pr√©dictions sur les donn√©es trait√©es.
"""

from pathlib import Path
import pickle
import joblib

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from loguru import logger
import typer

from corai.config import (
    PROCESSED_DATA_DIR, MODELS_DIR, REPORTS_DIR,
    DEFAULT_TEST_SIZE, DEFAULT_RANDOM_STATE, TARGET_COLUMN
)
from corai.preprocessing.data_loader import load_data, split_features_target
from corai.modeling.evaluate import ModelEvaluator

app = typer.Typer()


@app.command()
def demo(
    data_path: Path = PROCESSED_DATA_DIR / "processed_heart_disease_v0.csv",
    model_path: Path = MODELS_DIR / "heart_disease_model.pkl",
    test_size: float = None,
    random_state: int = None,
):
    """
    D√©monstration simple: charge le mod√®le existant et fait des pr√©dictions.
    
    Args:
        data_path: Chemin vers les donn√©es pr√©trait√©es
        model_path: Chemin vers le mod√®le entra√Æn√©
        test_size: Proportion pour le test (None = utilise DEFAULT_TEST_SIZE)
        random_state: Graine al√©atoire (None = utilise DEFAULT_RANDOM_STATE)
    """
    test_size = test_size or DEFAULT_TEST_SIZE
    random_state = random_state or DEFAULT_RANDOM_STATE
    
    logger.info("=" * 80)
    logger.info("üé¨ D√âMONSTRATION DU MOD√àLE EXISTANT")
    logger.info("=" * 80)
    
    # 1. Charger les donn√©es
    logger.info("Chargement des donn√©es pr√©trait√©es...")
    df = load_data(data_path)
    logger.info(f"Donn√©es charg√©es: {df.shape}")
    
    # 2. S√©parer features et target
    logger.info("S√©paration features/target...")
    X, y = split_features_target(df, target=TARGET_COLUMN)
    logger.info(f"Features: {X.shape}, Target: {y.shape}")
    
    # 3. Split train/test
    logger.info(f"S√©paration train/test ({int((1-test_size)*100)}/{int(test_size*100)})...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    logger.info(f"Train: {X_train.shape}, Test: {X_test.shape}")
    
    # 4. Charger le mod√®le
    logger.info(f"Chargement du mod√®le depuis: {model_path}")
    
    if not model_path.exists():
        logger.error(f"Mod√®le non trouv√©: {model_path}")
        logger.info("Ex√©cutez d'abord: python -m corai.modeling.train")
        return
    
    # Essayer de charger avec joblib d'abord (nouveau format BaseModel)
    try:
        model_data = joblib.load(model_path)
        
        # Format BaseModel: dict avec 'model', 'metadata', 'model_type'
        if isinstance(model_data, dict) and 'model' in model_data:
            model = model_data['model']
            model_type = model_data.get('model_type', 'unknown')
            metadata = model_data.get('metadata', {})
            logger.info(f"Mod√®le BaseModel charg√©: {model_type} (entra√Æn√© le {metadata.get('training_date', 'N/A')})")
        else:
            model = model_data
            logger.info("Mod√®le sklearn charg√© (joblib)")
    
    except Exception as e_joblib:
        # Essayer pickle (ancien format)
        logger.debug(f"Erreur joblib: {e_joblib}, tentative avec pickle...")
        try:
            with open(model_path, "rb") as f:
                model_data = pickle.load(f)
            
            if isinstance(model_data, dict):
                model = model_data.get("model")
                model_type = model_data.get("model_type", "unknown")
                logger.info(f"Mod√®le charg√© (pickle): {model_type}")
            else:
                model = model_data
                logger.info("Mod√®le charg√© (format ancien pickle)")
        except Exception as e_pickle:
            logger.error(f"Impossible de charger le mod√®le avec joblib ou pickle")
            raise RuntimeError(f"Erreur de chargement: joblib={e_joblib}, pickle={e_pickle}")
    
    # 5. Faire des pr√©dictions
    logger.info("Pr√©diction sur le test set...")
    y_pred = model.predict(X_test)
    
    # Calculer les probabilit√©s si disponible
    y_proba = None
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)
        logger.info("Probabilit√©s calcul√©es")
    
    # 6. √âvaluer les performances
    logger.info("=" * 80)
    logger.info("üìä R√âSULTATS DE L'√âVALUATION")
    logger.info("=" * 80)
    
    evaluator = ModelEvaluator(task_type="classification")
    metrics = evaluator.evaluate_classification(
        y_true=y_test.values,
        y_pred=y_pred,
        y_proba=y_proba
    )
    
    evaluator.print_metrics()
    
    # 7. Afficher quelques exemples de pr√©dictions
    logger.info("\n" + "=" * 80)
    logger.info("üîç EXEMPLES DE PR√âDICTIONS (10 premiers)")
    logger.info("=" * 80)
    
    results_df = pd.DataFrame({
        "Vrai": y_test.values[:10],
        "Pr√©dit": y_pred[:10],
        "Correct": y_test.values[:10] == y_pred[:10]
    })
    
    if y_proba is not None:
        results_df["Prob_0"] = y_proba[:10, 0]
        results_df["Prob_1"] = y_proba[:10, 1]
    
    logger.info("\n" + results_df.to_string())
    
    # 8. Sauvegarder les r√©sultats
    output_dir = REPORTS_DIR / "demo_results"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Sauvegarder les pr√©dictions compl√®tes
    full_results = pd.DataFrame({
        "true_label": y_test.values,
        "predicted_label": y_pred,
        "correct": y_test.values == y_pred
    })
    
    if y_proba is not None:
        full_results["probability_0"] = y_proba[:, 0]
        full_results["probability_1"] = y_proba[:, 1]
    
    results_path = output_dir / "predictions_demo.csv"
    full_results.to_csv(results_path, index=False)
    logger.info(f"\nPr√©dictions sauvegard√©es: {results_path}")
    
    # Sauvegarder les m√©triques
    metrics_path = output_dir / "metrics_demo.json"
    evaluator.save_metrics(metrics_path)
    
    # 9. R√©sum√© final
    logger.info("\n" + "=" * 80)
    logger.info("‚úÖ R√âSUM√â")
    logger.info("=" * 80)
    logger.info(f"Pr√©cision globale: {metrics['accuracy']:.2%}")
    logger.info(f"F1 Score: {metrics['f1_score']:.4f}")
    
    correct = (y_test.values == y_pred).sum()
    incorrect = len(y_test) - correct
    logger.info(f"Pr√©dictions correctes: {correct}/{len(y_test)}")
    logger.info(f"Pr√©dictions incorrectes: {incorrect}/{len(y_test)}")
    
    typer.echo("\n‚úÖ D√©monstration termin√©e!")


if __name__ == "__main__":
    app()


# Usage:
# python -m corai.demo_model
